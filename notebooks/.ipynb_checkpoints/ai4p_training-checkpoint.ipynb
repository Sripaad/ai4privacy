{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f04c7bb-f80a-47ad-a655-69f396233278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de6dea-f7ca-4851-8e8d-8bac1e27fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cd8e2a-15f8-446c-acc0-38f7ec29db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-large-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d974bf8e-286e-4309-94e6-d82899d67ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df1 = pd.read_json(\"data/english_balanced_10k.jsonl\", lines=True)\n",
    "# df2 = pd.read_json(\"data/PII43k_original.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03aa9d87-2e12-4542-805a-a66722f8a8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df1.append([df2]).reset_index(drop=True)\n",
    "# df = df.rename(columns={\n",
    "#     \"Masked text\" : \"target_text\",\n",
    "#     \"Unmasked text\": \"source_text\",\n",
    "#     \"Tokenised Masked text\": \"tokenized_text\",\n",
    "#     \"Tokensised Unmasked text\": \"ner_tags\", # need it to find all present labels\n",
    "# }) # why add spaces in column names?\n",
    "# # df = df.drop_duplicates()\n",
    "# df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea950da0-442d-4878-a39b-b910ebb3472f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_text</th>\n",
       "      <th>source_text</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PREFIX_1] [FIRSTNAME_1] [LASTNAME_1], please ...</td>\n",
       "      <td>Ms. Savion Von, please do not forget your appo...</td>\n",
       "      <td>[B-PREFIX, I-PREFIX, B-FIRSTNAME, I-FIRSTNAME,...</td>\n",
       "      <td>[ms, ., sa, ##vio, ##n, von, ,, please, do, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support group meeting for [GENDER_1] affected ...</td>\n",
       "      <td>Support group meeting for Transexual person af...</td>\n",
       "      <td>[O, O, O, O, B-GENDER, I-GENDER, I-GENDER, I-G...</td>\n",
       "      <td>[support, group, meeting, for, trans, ##ex, ##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[FIRSTNAME_1], this is a reminder about your p...</td>\n",
       "      <td>Trycia, this is a reminder about your psycho-o...</td>\n",
       "      <td>[B-FIRSTNAME, I-FIRSTNAME, O, O, O, O, O, O, O...</td>\n",
       "      <td>[try, ##cia, ,, this, is, a, reminder, about, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[USERNAME_1], please visit our page [URL_1] to...</td>\n",
       "      <td>Juliet.Murazik85, please visit our page https:...</td>\n",
       "      <td>[B-USERNAME, I-USERNAME, I-USERNAME, I-USERNAM...</td>\n",
       "      <td>[juliet, ., mu, ##raz, ##ik, ##85, ,, please, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psycho-oncology research forum, [DATE_1] at [T...</td>\n",
       "      <td>Psycho-oncology research forum, 2/11 at 9 AM, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-DATE, I-DATE, I-DATE, ...</td>\n",
       "      <td>[psycho, -, on, ##cology, research, forum, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59387</th>\n",
       "      <td>Can you please provide a breakdown of the comp...</td>\n",
       "      <td>Can you please provide a breakdown of the comp...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[can, you, please, provide, a, breakdown, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59388</th>\n",
       "      <td>A transaction for user [USERNAME_1] on [DATE_1...</td>\n",
       "      <td>A transaction for user Patsy_Volkman on 23/10/...</td>\n",
       "      <td>[O, O, O, O, B-USERNAME, I-USERNAME, I-USERNAM...</td>\n",
       "      <td>[a, transaction, for, user, patsy, _, vol, ##k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59389</th>\n",
       "      <td>We are curious about the current investments i...</td>\n",
       "      <td>We are curious about the current investments i...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-STATE, O, O, O, O, ...</td>\n",
       "      <td>[we, are, curious, about, the, current, invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59390</th>\n",
       "      <td>Can you create an update presentation about th...</td>\n",
       "      <td>Can you create an update presentation about th...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[can, you, create, an, update, presentation, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59391</th>\n",
       "      <td>Can you help with the documentation to set up ...</td>\n",
       "      <td>Can you help with the documentation to set up ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-C...</td>\n",
       "      <td>[can, you, help, with, the, documentation, to,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59392 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             target_text  \\\n",
       "0      [PREFIX_1] [FIRSTNAME_1] [LASTNAME_1], please ...   \n",
       "1      Support group meeting for [GENDER_1] affected ...   \n",
       "2      [FIRSTNAME_1], this is a reminder about your p...   \n",
       "3      [USERNAME_1], please visit our page [URL_1] to...   \n",
       "4      Psycho-oncology research forum, [DATE_1] at [T...   \n",
       "...                                                  ...   \n",
       "59387  Can you please provide a breakdown of the comp...   \n",
       "59388  A transaction for user [USERNAME_1] on [DATE_1...   \n",
       "59389  We are curious about the current investments i...   \n",
       "59390  Can you create an update presentation about th...   \n",
       "59391  Can you help with the documentation to set up ...   \n",
       "\n",
       "                                             source_text  \\\n",
       "0      Ms. Savion Von, please do not forget your appo...   \n",
       "1      Support group meeting for Transexual person af...   \n",
       "2      Trycia, this is a reminder about your psycho-o...   \n",
       "3      Juliet.Murazik85, please visit our page https:...   \n",
       "4      Psycho-oncology research forum, 2/11 at 9 AM, ...   \n",
       "...                                                  ...   \n",
       "59387  Can you please provide a breakdown of the comp...   \n",
       "59388  A transaction for user Patsy_Volkman on 23/10/...   \n",
       "59389  We are curious about the current investments i...   \n",
       "59390  Can you create an update presentation about th...   \n",
       "59391  Can you help with the documentation to set up ...   \n",
       "\n",
       "                                                ner_tags  \\\n",
       "0      [B-PREFIX, I-PREFIX, B-FIRSTNAME, I-FIRSTNAME,...   \n",
       "1      [O, O, O, O, B-GENDER, I-GENDER, I-GENDER, I-G...   \n",
       "2      [B-FIRSTNAME, I-FIRSTNAME, O, O, O, O, O, O, O...   \n",
       "3      [B-USERNAME, I-USERNAME, I-USERNAME, I-USERNAM...   \n",
       "4      [O, O, O, O, O, O, O, B-DATE, I-DATE, I-DATE, ...   \n",
       "...                                                  ...   \n",
       "59387  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "59388  [O, O, O, O, B-USERNAME, I-USERNAME, I-USERNAM...   \n",
       "59389  [O, O, O, O, O, O, O, O, B-STATE, O, O, O, O, ...   \n",
       "59390  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "59391  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-C...   \n",
       "\n",
       "                                          tokenized_text  \n",
       "0      [ms, ., sa, ##vio, ##n, von, ,, please, do, no...  \n",
       "1      [support, group, meeting, for, trans, ##ex, ##...  \n",
       "2      [try, ##cia, ,, this, is, a, reminder, about, ...  \n",
       "3      [juliet, ., mu, ##raz, ##ik, ##85, ,, please, ...  \n",
       "4      [psycho, -, on, ##cology, research, forum, ,, ...  \n",
       "...                                                  ...  \n",
       "59387  [can, you, please, provide, a, breakdown, of, ...  \n",
       "59388  [a, transaction, for, user, patsy, _, vol, ##k...  \n",
       "59389  [we, are, curious, about, the, current, invest...  \n",
       "59390  [can, you, create, an, update, presentation, a...  \n",
       "59391  [can, you, help, with, the, documentation, to,...  \n",
       "\n",
       "[59392 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"../data/pii200k_english.jsonl\", lines=True)\n",
    "df = df.rename(columns={\n",
    "    \"masked_text\" : \"target_text\",\n",
    "    \"unmasked_text\": \"source_text\",\n",
    "    \"tokenised_unmasked_text\": \"tokenized_text\",\n",
    "    \"token_entity_labels\": \"ner_tags\", # need it to find all present labels\n",
    "})\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3b0dc-e33e-4653-8150-c16d006e1aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4770cefa-a1b3-4c61-b224-d353ac01b4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1c7fac95114a728be0038c8a4bacce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove more than 512\n",
    "tt = df.tokenized_text.tolist()\n",
    "tt_lens = [len(tokenizer(t, is_split_into_words=True, truncation=True, max_length=512)) for t in tqdm(tt)]\n",
    "df[['tt_lens']] = 0\n",
    "df.tt_lens = tt_lens\n",
    "df = df.loc[df.tt_lens <= 510].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37ec40-2be2-4d0d-a273-0d6bc433d3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nt = df.ner_tags.tolist()\n",
    "nt = list(itertools.chain.from_iterable(nt)) # merge the list of lists into one list\n",
    "nt = collections.Counter(nt) # Get count of each tag\n",
    "all_labels = list(nt.keys()) # get all unique tags(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f136e5f7-672e-46cd-9968-a0d4c4f64d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_texts = df.source_text.tolist()\n",
    "target_texts = df.target_text.tolist()\n",
    "tokenized_texts = df.tokenized_text.tolist()\n",
    "ner_tags = df.ner_tags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e7b39c-1cc9-4e25-9700-aa9ce6c3fd93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014d7576-4c49-47d1-aec3-19199981e606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ms. Savion Von, please do not forget your appointment on 28th November at 09. Keep our 69-616376-555417-0 and Gilbert_Dooley in mind.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283387ee-e978-449d-9492-237b0eb2f957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PREFIX_1] [FIRSTNAME_1] [LASTNAME_1], please do not forget your appointment on [DATE_1] at [TIME_1]. Keep our [PHONEIMEI_1] and [USERNAME_1] in mind.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ced38fcf-014d-4142-959a-048e5a1b8eac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('enroll', 'enroll')\n",
      "('patient', 'patient')\n",
      "('blair', 'blair')\n",
      "('_', '_')\n",
      "('den', 'den')\n",
      "('##es', '##es')\n",
      "('##ik', '##ik')\n",
      "('-', '-')\n",
      "('lu', 'lu')\n",
      "('##bow', '##bow')\n",
      "('##itz', '##itz')\n",
      "('##44', '##44')\n",
      "('in', 'in')\n",
      "('trial', 'trial')\n",
      "('id', 'id')\n",
      "('580', '580')\n",
      "('##14', '##14')\n",
      "('##41', '##41')\n",
      "('##44', '##44')\n",
      "('##36', '##36')\n",
      "('##75', '##75')\n",
      "('##7', '##7')\n",
      "('##70', '##70')\n",
      "('.', '.')\n",
      "('sex', 'sex')\n",
      "(':', ':')\n",
      "('female', 'female')\n",
      "(',', ',')\n",
      "('do', 'do')\n",
      "('##b', '##b')\n",
      "(':', ':')\n",
      "('october', 'october')\n",
      "('6', '6')\n",
      "(',', ',')\n",
      "('1952', '1952')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "## checking if the tokens align\n",
    "i = random.randint(0, len(source_texts))\n",
    "x0 = tokenizer.convert_ids_to_tokens(tokenizer(source_texts[i])['input_ids'])\n",
    "x0.pop(0)  # CLS is not present in the dataset\n",
    "for t in zip(x0, tokenized_texts[i]):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "442dcbb1-e25e-4e2e-bec9-0e4e7520487d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('O', 'enroll')\n",
      "('O', 'patient')\n",
      "('B-USERNAME', 'blair')\n",
      "('I-USERNAME', '_')\n",
      "('I-USERNAME', 'den')\n",
      "('I-USERNAME', '##es')\n",
      "('I-USERNAME', '##ik')\n",
      "('I-USERNAME', '-')\n",
      "('I-USERNAME', 'lu')\n",
      "('I-USERNAME', '##bow')\n",
      "('I-USERNAME', '##itz')\n",
      "('I-USERNAME', '##44')\n",
      "('O', 'in')\n",
      "('O', 'trial')\n",
      "('O', 'id')\n",
      "('B-MASKEDNUMBER', '580')\n",
      "('I-MASKEDNUMBER', '##14')\n",
      "('I-MASKEDNUMBER', '##41')\n",
      "('I-MASKEDNUMBER', '##44')\n",
      "('I-MASKEDNUMBER', '##36')\n",
      "('I-MASKEDNUMBER', '##75')\n",
      "('I-MASKEDNUMBER', '##7')\n",
      "('I-MASKEDNUMBER', '##70')\n",
      "('O', '.')\n",
      "('O', 'sex')\n",
      "('O', ':')\n",
      "('B-SEX', 'female')\n",
      "('O', ',')\n",
      "('O', 'do')\n",
      "('O', '##b')\n",
      "('O', ':')\n",
      "('B-DOB', 'october')\n",
      "('I-DOB', '6')\n",
      "('I-DOB', ',')\n",
      "('I-DOB', '1952')\n",
      "('O', '.')\n"
     ]
    }
   ],
   "source": [
    "# Checking if the tokens align with tags\n",
    "# i = random.randint(0, len(target_texts))\n",
    "for t in zip(ner_tags[i], tokenized_texts[i]):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3848bd-bac9-4816-a7cf-a98f6aef36c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'B-PREFIX': 0,\n",
       "  'I-PREFIX': 1,\n",
       "  'B-FIRSTNAME': 2,\n",
       "  'I-FIRSTNAME': 3,\n",
       "  'B-LASTNAME': 4,\n",
       "  'O': 5,\n",
       "  'B-DATE': 6,\n",
       "  'I-DATE': 7,\n",
       "  'B-TIME': 8,\n",
       "  'B-PHONEIMEI': 9,\n",
       "  'I-PHONEIMEI': 10,\n",
       "  'B-USERNAME': 11,\n",
       "  'I-USERNAME': 12,\n",
       "  'B-GENDER': 13,\n",
       "  'I-GENDER': 14,\n",
       "  'B-CITY': 15,\n",
       "  'I-CITY': 16,\n",
       "  'B-STATE': 17,\n",
       "  'B-URL': 18,\n",
       "  'I-URL': 19,\n",
       "  'B-JOBAREA': 20,\n",
       "  'I-TIME': 21,\n",
       "  'B-EMAIL': 22,\n",
       "  'I-EMAIL': 23,\n",
       "  'B-JOBTYPE': 24,\n",
       "  'I-LASTNAME': 25,\n",
       "  'B-COMPANYNAME': 26,\n",
       "  'I-COMPANYNAME': 27,\n",
       "  'B-JOBTITLE': 28,\n",
       "  'I-JOBTITLE': 29,\n",
       "  'B-STREET': 30,\n",
       "  'I-STREET': 31,\n",
       "  'B-SECONDARYADDRESS': 32,\n",
       "  'I-SECONDARYADDRESS': 33,\n",
       "  'B-COUNTY': 34,\n",
       "  'I-COUNTY': 35,\n",
       "  'B-AGE': 36,\n",
       "  'I-AGE': 37,\n",
       "  'B-USERAGENT': 38,\n",
       "  'I-USERAGENT': 39,\n",
       "  'B-ACCOUNTNAME': 40,\n",
       "  'I-ACCOUNTNAME': 41,\n",
       "  'B-ACCOUNTNUMBER': 42,\n",
       "  'I-ACCOUNTNUMBER': 43,\n",
       "  'B-CURRENCYSYMBOL': 44,\n",
       "  'I-CURRENCYSYMBOL': 45,\n",
       "  'B-AMOUNT': 46,\n",
       "  'I-AMOUNT': 47,\n",
       "  'B-CREDITCARDISSUER': 48,\n",
       "  'B-CREDITCARDNUMBER': 49,\n",
       "  'I-CREDITCARDNUMBER': 50,\n",
       "  'B-CREDITCARDCVV': 51,\n",
       "  'B-PHONENUMBER': 52,\n",
       "  'I-PHONENUMBER': 53,\n",
       "  'B-SEX': 54,\n",
       "  'I-CREDITCARDISSUER': 55,\n",
       "  'B-IP': 56,\n",
       "  'I-IP': 57,\n",
       "  'B-ETHEREUMADDRESS': 58,\n",
       "  'I-ETHEREUMADDRESS': 59,\n",
       "  'B-BITCOINADDRESS': 60,\n",
       "  'I-BITCOINADDRESS': 61,\n",
       "  'B-MIDDLENAME': 62,\n",
       "  'B-IBAN': 63,\n",
       "  'I-IBAN': 64,\n",
       "  'B-VEHICLEVRM': 65,\n",
       "  'I-VEHICLEVRM': 66,\n",
       "  'B-DOB': 67,\n",
       "  'I-DOB': 68,\n",
       "  'B-PIN': 69,\n",
       "  'I-PIN': 70,\n",
       "  'B-CURRENCY': 71,\n",
       "  'I-CURRENCY': 72,\n",
       "  'I-CREDITCARDCVV': 73,\n",
       "  'B-PASSWORD': 74,\n",
       "  'I-PASSWORD': 75,\n",
       "  'B-CURRENCYNAME': 76,\n",
       "  'I-CURRENCYNAME': 77,\n",
       "  'B-LITECOINADDRESS': 78,\n",
       "  'I-LITECOINADDRESS': 79,\n",
       "  'B-CURRENCYCODE': 80,\n",
       "  'I-CURRENCYCODE': 81,\n",
       "  'B-BUILDINGNUMBER': 82,\n",
       "  'I-BUILDINGNUMBER': 83,\n",
       "  'B-ORDINALDIRECTION': 84,\n",
       "  'B-MASKEDNUMBER': 85,\n",
       "  'I-MASKEDNUMBER': 86,\n",
       "  'B-ZIPCODE': 87,\n",
       "  'I-ZIPCODE': 88,\n",
       "  'B-BIC': 89,\n",
       "  'I-BIC': 90,\n",
       "  'B-IPV4': 91,\n",
       "  'I-IPV4': 92,\n",
       "  'I-JOBAREA': 93,\n",
       "  'I-MIDDLENAME': 94,\n",
       "  'B-IPV6': 95,\n",
       "  'I-IPV6': 96,\n",
       "  'B-MAC': 97,\n",
       "  'I-MAC': 98,\n",
       "  'I-JOBTYPE': 99,\n",
       "  'B-NEARBYGPSCOORDINATE': 100,\n",
       "  'I-NEARBYGPSCOORDINATE': 101,\n",
       "  'B-VEHICLEVIN': 102,\n",
       "  'I-VEHICLEVIN': 103,\n",
       "  'B-EYECOLOR': 104,\n",
       "  'I-EYECOLOR': 105,\n",
       "  'B-HEIGHT': 106,\n",
       "  'I-HEIGHT': 107,\n",
       "  'I-STATE': 108,\n",
       "  'B-SSN': 109,\n",
       "  'I-SSN': 110,\n",
       "  'B-JOBDESCRIPTOR': 111,\n",
       "  'B-COMPANY_NAME': 112,\n",
       "  'I-COMPANY_NAME': 113,\n",
       "  'B-STREETADDRESS': 114,\n",
       "  'I-STREETADDRESS': 115,\n",
       "  'B-SUFFIX': 116,\n",
       "  'I-SUFFIX': 117,\n",
       "  'B-PHONE_NUMBER': 118,\n",
       "  'I-PHONE_NUMBER': 119,\n",
       "  'B-FULLNAME': 120,\n",
       "  'I-FULLNAME': 121},\n",
       " {0: 'B-PREFIX',\n",
       "  1: 'I-PREFIX',\n",
       "  2: 'B-FIRSTNAME',\n",
       "  3: 'I-FIRSTNAME',\n",
       "  4: 'B-LASTNAME',\n",
       "  5: 'O',\n",
       "  6: 'B-DATE',\n",
       "  7: 'I-DATE',\n",
       "  8: 'B-TIME',\n",
       "  9: 'B-PHONEIMEI',\n",
       "  10: 'I-PHONEIMEI',\n",
       "  11: 'B-USERNAME',\n",
       "  12: 'I-USERNAME',\n",
       "  13: 'B-GENDER',\n",
       "  14: 'I-GENDER',\n",
       "  15: 'B-CITY',\n",
       "  16: 'I-CITY',\n",
       "  17: 'B-STATE',\n",
       "  18: 'B-URL',\n",
       "  19: 'I-URL',\n",
       "  20: 'B-JOBAREA',\n",
       "  21: 'I-TIME',\n",
       "  22: 'B-EMAIL',\n",
       "  23: 'I-EMAIL',\n",
       "  24: 'B-JOBTYPE',\n",
       "  25: 'I-LASTNAME',\n",
       "  26: 'B-COMPANYNAME',\n",
       "  27: 'I-COMPANYNAME',\n",
       "  28: 'B-JOBTITLE',\n",
       "  29: 'I-JOBTITLE',\n",
       "  30: 'B-STREET',\n",
       "  31: 'I-STREET',\n",
       "  32: 'B-SECONDARYADDRESS',\n",
       "  33: 'I-SECONDARYADDRESS',\n",
       "  34: 'B-COUNTY',\n",
       "  35: 'I-COUNTY',\n",
       "  36: 'B-AGE',\n",
       "  37: 'I-AGE',\n",
       "  38: 'B-USERAGENT',\n",
       "  39: 'I-USERAGENT',\n",
       "  40: 'B-ACCOUNTNAME',\n",
       "  41: 'I-ACCOUNTNAME',\n",
       "  42: 'B-ACCOUNTNUMBER',\n",
       "  43: 'I-ACCOUNTNUMBER',\n",
       "  44: 'B-CURRENCYSYMBOL',\n",
       "  45: 'I-CURRENCYSYMBOL',\n",
       "  46: 'B-AMOUNT',\n",
       "  47: 'I-AMOUNT',\n",
       "  48: 'B-CREDITCARDISSUER',\n",
       "  49: 'B-CREDITCARDNUMBER',\n",
       "  50: 'I-CREDITCARDNUMBER',\n",
       "  51: 'B-CREDITCARDCVV',\n",
       "  52: 'B-PHONENUMBER',\n",
       "  53: 'I-PHONENUMBER',\n",
       "  54: 'B-SEX',\n",
       "  55: 'I-CREDITCARDISSUER',\n",
       "  56: 'B-IP',\n",
       "  57: 'I-IP',\n",
       "  58: 'B-ETHEREUMADDRESS',\n",
       "  59: 'I-ETHEREUMADDRESS',\n",
       "  60: 'B-BITCOINADDRESS',\n",
       "  61: 'I-BITCOINADDRESS',\n",
       "  62: 'B-MIDDLENAME',\n",
       "  63: 'B-IBAN',\n",
       "  64: 'I-IBAN',\n",
       "  65: 'B-VEHICLEVRM',\n",
       "  66: 'I-VEHICLEVRM',\n",
       "  67: 'B-DOB',\n",
       "  68: 'I-DOB',\n",
       "  69: 'B-PIN',\n",
       "  70: 'I-PIN',\n",
       "  71: 'B-CURRENCY',\n",
       "  72: 'I-CURRENCY',\n",
       "  73: 'I-CREDITCARDCVV',\n",
       "  74: 'B-PASSWORD',\n",
       "  75: 'I-PASSWORD',\n",
       "  76: 'B-CURRENCYNAME',\n",
       "  77: 'I-CURRENCYNAME',\n",
       "  78: 'B-LITECOINADDRESS',\n",
       "  79: 'I-LITECOINADDRESS',\n",
       "  80: 'B-CURRENCYCODE',\n",
       "  81: 'I-CURRENCYCODE',\n",
       "  82: 'B-BUILDINGNUMBER',\n",
       "  83: 'I-BUILDINGNUMBER',\n",
       "  84: 'B-ORDINALDIRECTION',\n",
       "  85: 'B-MASKEDNUMBER',\n",
       "  86: 'I-MASKEDNUMBER',\n",
       "  87: 'B-ZIPCODE',\n",
       "  88: 'I-ZIPCODE',\n",
       "  89: 'B-BIC',\n",
       "  90: 'I-BIC',\n",
       "  91: 'B-IPV4',\n",
       "  92: 'I-IPV4',\n",
       "  93: 'I-JOBAREA',\n",
       "  94: 'I-MIDDLENAME',\n",
       "  95: 'B-IPV6',\n",
       "  96: 'I-IPV6',\n",
       "  97: 'B-MAC',\n",
       "  98: 'I-MAC',\n",
       "  99: 'I-JOBTYPE',\n",
       "  100: 'B-NEARBYGPSCOORDINATE',\n",
       "  101: 'I-NEARBYGPSCOORDINATE',\n",
       "  102: 'B-VEHICLEVIN',\n",
       "  103: 'I-VEHICLEVIN',\n",
       "  104: 'B-EYECOLOR',\n",
       "  105: 'I-EYECOLOR',\n",
       "  106: 'B-HEIGHT',\n",
       "  107: 'I-HEIGHT',\n",
       "  108: 'I-STATE',\n",
       "  109: 'B-SSN',\n",
       "  110: 'I-SSN',\n",
       "  111: 'B-JOBDESCRIPTOR',\n",
       "  112: 'B-COMPANY_NAME',\n",
       "  113: 'I-COMPANY_NAME',\n",
       "  114: 'B-STREETADDRESS',\n",
       "  115: 'I-STREETADDRESS',\n",
       "  116: 'B-SUFFIX',\n",
       "  117: 'I-SUFFIX',\n",
       "  118: 'B-PHONE_NUMBER',\n",
       "  119: 'I-PHONE_NUMBER',\n",
       "  120: 'B-FULLNAME',\n",
       "  121: 'I-FULLNAME'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create label dict\n",
    "label2id = dict([(value,key) for key, value in enumerate(all_labels)])\n",
    "id2label = dict(map(reversed, label2id.items()))\n",
    "\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc6bedb7-1960-49e5-b722-230a7746d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499368664bf744cc856070af1f64d86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in tqdm(range(len(ner_tags))):\n",
    "    tags = ner_tags[j]\n",
    "    for i in range(len(tags)):\n",
    "        for k,v in label2id.items():\n",
    "            if tags[i] == k:\n",
    "                tags[i] = v\n",
    "    ner_tags[j] = tags\n",
    "df.ner_tags = ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "740458df-191c-4e02-8089-aa370b0e5fb3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 5,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff23a079-cdce-4818-a27f-b55f855f5f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['source_words']] = \"source_words\"\n",
    "source_words = [text.split(\" \") for text in source_texts]\n",
    "df.source_words = source_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae8d23e-6507-4f44-929a-52d3bb70cbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing rows where the len(tokenized_texts[i]) does not match len(ner_tags[i])\n",
    "idx = [i for i in range(len(ner_tags)) if len(tokenized_texts[i]) != len(ner_tags[i])]\n",
    "df = df.drop(index=idx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7cc0ae-0742-4a8d-91fe-61a35ec33603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target_text', 'source_text', 'ner_tags', 'tokenized_text', 'tt_lens', 'source_words'],\n",
       "    num_rows: 58113\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "951e9dd6-5e09-48d7-9d09-529bc33af07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_labels(example):\n",
    "    tokenized_input = tokenizer(example[\"tokenized_text\"], is_split_into_words=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    aligned_labels = [-100 if i is None else example[\"ner_tags\"][i] for i in word_ids]\n",
    "    tokenized_input['labels'] = aligned_labels\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db8f2d7-4180-4aee-ad80-badf03240963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60 60\n"
     ]
    }
   ],
   "source": [
    "al = align_labels(dataset[0])\n",
    "print(len(al['input_ids']), len(al['attention_mask']), len(al['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d4d678b-c154-4f41-8f0b-83f0a0b08fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokenized_text\"], is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ae6e964-d915-488b-b03a-cf262096cb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565ed89a83f74853ad8bd5a545ff275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/58113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (997 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "x = dataset.map(align_labels, num_proc=8, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2d5d4c6-5d52-4612-bce9-0c65a98181b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = x.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8732f21-bcd6-47bb-839a-13e66b84c6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 46490\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11623\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e03ae609-8ce5-4521-b8e5-f707267e7b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07f3f6fa-b381-4719-bbc5-556edf2731a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/7m8x2m5s2qv5xkd9zgpmblv00000gn/T/ipykernel_10456/1110732161.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = datasets.load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "    for k in results.keys():\n",
    "        if (k not in flattened_results.keys()):\n",
    "            flattened_results[f\"{k}_f1\"] = results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9027a465-848e-4be9-aa5d-1efd90aad2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-large-generator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-large-generator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 1024,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-PREFIX\",\n",
      "    \"1\": \"I-PREFIX\",\n",
      "    \"2\": \"B-FIRSTNAME\",\n",
      "    \"3\": \"I-FIRSTNAME\",\n",
      "    \"4\": \"B-LASTNAME\",\n",
      "    \"5\": \"O\",\n",
      "    \"6\": \"B-DATE\",\n",
      "    \"7\": \"I-DATE\",\n",
      "    \"8\": \"B-TIME\",\n",
      "    \"9\": \"B-PHONEIMEI\",\n",
      "    \"10\": \"I-PHONEIMEI\",\n",
      "    \"11\": \"B-USERNAME\",\n",
      "    \"12\": \"I-USERNAME\",\n",
      "    \"13\": \"B-GENDER\",\n",
      "    \"14\": \"I-GENDER\",\n",
      "    \"15\": \"B-CITY\",\n",
      "    \"16\": \"I-CITY\",\n",
      "    \"17\": \"B-STATE\",\n",
      "    \"18\": \"B-URL\",\n",
      "    \"19\": \"I-URL\",\n",
      "    \"20\": \"B-JOBAREA\",\n",
      "    \"21\": \"I-TIME\",\n",
      "    \"22\": \"B-EMAIL\",\n",
      "    \"23\": \"I-EMAIL\",\n",
      "    \"24\": \"B-JOBTYPE\",\n",
      "    \"25\": \"I-LASTNAME\",\n",
      "    \"26\": \"B-COMPANYNAME\",\n",
      "    \"27\": \"I-COMPANYNAME\",\n",
      "    \"28\": \"B-JOBTITLE\",\n",
      "    \"29\": \"I-JOBTITLE\",\n",
      "    \"30\": \"B-STREET\",\n",
      "    \"31\": \"I-STREET\",\n",
      "    \"32\": \"B-SECONDARYADDRESS\",\n",
      "    \"33\": \"I-SECONDARYADDRESS\",\n",
      "    \"34\": \"B-COUNTY\",\n",
      "    \"35\": \"I-COUNTY\",\n",
      "    \"36\": \"B-AGE\",\n",
      "    \"37\": \"I-AGE\",\n",
      "    \"38\": \"B-USERAGENT\",\n",
      "    \"39\": \"I-USERAGENT\",\n",
      "    \"40\": \"B-ACCOUNTNAME\",\n",
      "    \"41\": \"I-ACCOUNTNAME\",\n",
      "    \"42\": \"B-ACCOUNTNUMBER\",\n",
      "    \"43\": \"I-ACCOUNTNUMBER\",\n",
      "    \"44\": \"B-CURRENCYSYMBOL\",\n",
      "    \"45\": \"I-CURRENCYSYMBOL\",\n",
      "    \"46\": \"B-AMOUNT\",\n",
      "    \"47\": \"I-AMOUNT\",\n",
      "    \"48\": \"B-CREDITCARDISSUER\",\n",
      "    \"49\": \"B-CREDITCARDNUMBER\",\n",
      "    \"50\": \"I-CREDITCARDNUMBER\",\n",
      "    \"51\": \"B-CREDITCARDCVV\",\n",
      "    \"52\": \"B-PHONENUMBER\",\n",
      "    \"53\": \"I-PHONENUMBER\",\n",
      "    \"54\": \"B-SEX\",\n",
      "    \"55\": \"I-CREDITCARDISSUER\",\n",
      "    \"56\": \"B-IP\",\n",
      "    \"57\": \"I-IP\",\n",
      "    \"58\": \"B-ETHEREUMADDRESS\",\n",
      "    \"59\": \"I-ETHEREUMADDRESS\",\n",
      "    \"60\": \"B-BITCOINADDRESS\",\n",
      "    \"61\": \"I-BITCOINADDRESS\",\n",
      "    \"62\": \"B-MIDDLENAME\",\n",
      "    \"63\": \"B-IBAN\",\n",
      "    \"64\": \"I-IBAN\",\n",
      "    \"65\": \"B-VEHICLEVRM\",\n",
      "    \"66\": \"I-VEHICLEVRM\",\n",
      "    \"67\": \"B-DOB\",\n",
      "    \"68\": \"I-DOB\",\n",
      "    \"69\": \"B-PIN\",\n",
      "    \"70\": \"I-PIN\",\n",
      "    \"71\": \"B-CURRENCY\",\n",
      "    \"72\": \"I-CURRENCY\",\n",
      "    \"73\": \"I-CREDITCARDCVV\",\n",
      "    \"74\": \"B-PASSWORD\",\n",
      "    \"75\": \"I-PASSWORD\",\n",
      "    \"76\": \"B-CURRENCYNAME\",\n",
      "    \"77\": \"I-CURRENCYNAME\",\n",
      "    \"78\": \"B-LITECOINADDRESS\",\n",
      "    \"79\": \"I-LITECOINADDRESS\",\n",
      "    \"80\": \"B-CURRENCYCODE\",\n",
      "    \"81\": \"I-CURRENCYCODE\",\n",
      "    \"82\": \"B-BUILDINGNUMBER\",\n",
      "    \"83\": \"I-BUILDINGNUMBER\",\n",
      "    \"84\": \"B-ORDINALDIRECTION\",\n",
      "    \"85\": \"B-MASKEDNUMBER\",\n",
      "    \"86\": \"I-MASKEDNUMBER\",\n",
      "    \"87\": \"B-ZIPCODE\",\n",
      "    \"88\": \"I-ZIPCODE\",\n",
      "    \"89\": \"B-BIC\",\n",
      "    \"90\": \"I-BIC\",\n",
      "    \"91\": \"B-IPV4\",\n",
      "    \"92\": \"I-IPV4\",\n",
      "    \"93\": \"I-JOBAREA\",\n",
      "    \"94\": \"I-MIDDLENAME\",\n",
      "    \"95\": \"B-IPV6\",\n",
      "    \"96\": \"I-IPV6\",\n",
      "    \"97\": \"B-MAC\",\n",
      "    \"98\": \"I-MAC\",\n",
      "    \"99\": \"I-JOBTYPE\",\n",
      "    \"100\": \"B-NEARBYGPSCOORDINATE\",\n",
      "    \"101\": \"I-NEARBYGPSCOORDINATE\",\n",
      "    \"102\": \"B-VEHICLEVIN\",\n",
      "    \"103\": \"I-VEHICLEVIN\",\n",
      "    \"104\": \"B-EYECOLOR\",\n",
      "    \"105\": \"I-EYECOLOR\",\n",
      "    \"106\": \"B-HEIGHT\",\n",
      "    \"107\": \"I-HEIGHT\",\n",
      "    \"108\": \"I-STATE\",\n",
      "    \"109\": \"B-SSN\",\n",
      "    \"110\": \"I-SSN\",\n",
      "    \"111\": \"B-JOBDESCRIPTOR\",\n",
      "    \"112\": \"B-COMPANY_NAME\",\n",
      "    \"113\": \"I-COMPANY_NAME\",\n",
      "    \"114\": \"B-STREETADDRESS\",\n",
      "    \"115\": \"I-STREETADDRESS\",\n",
      "    \"116\": \"B-SUFFIX\",\n",
      "    \"117\": \"I-SUFFIX\",\n",
      "    \"118\": \"B-PHONE_NUMBER\",\n",
      "    \"119\": \"I-PHONE_NUMBER\",\n",
      "    \"120\": \"B-FULLNAME\",\n",
      "    \"121\": \"I-FULLNAME\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"B-ACCOUNTNAME\": 40,\n",
      "    \"B-ACCOUNTNUMBER\": 42,\n",
      "    \"B-AGE\": 36,\n",
      "    \"B-AMOUNT\": 46,\n",
      "    \"B-BIC\": 89,\n",
      "    \"B-BITCOINADDRESS\": 60,\n",
      "    \"B-BUILDINGNUMBER\": 82,\n",
      "    \"B-CITY\": 15,\n",
      "    \"B-COMPANYNAME\": 26,\n",
      "    \"B-COMPANY_NAME\": 112,\n",
      "    \"B-COUNTY\": 34,\n",
      "    \"B-CREDITCARDCVV\": 51,\n",
      "    \"B-CREDITCARDISSUER\": 48,\n",
      "    \"B-CREDITCARDNUMBER\": 49,\n",
      "    \"B-CURRENCY\": 71,\n",
      "    \"B-CURRENCYCODE\": 80,\n",
      "    \"B-CURRENCYNAME\": 76,\n",
      "    \"B-CURRENCYSYMBOL\": 44,\n",
      "    \"B-DATE\": 6,\n",
      "    \"B-DOB\": 67,\n",
      "    \"B-EMAIL\": 22,\n",
      "    \"B-ETHEREUMADDRESS\": 58,\n",
      "    \"B-EYECOLOR\": 104,\n",
      "    \"B-FIRSTNAME\": 2,\n",
      "    \"B-FULLNAME\": 120,\n",
      "    \"B-GENDER\": 13,\n",
      "    \"B-HEIGHT\": 106,\n",
      "    \"B-IBAN\": 63,\n",
      "    \"B-IP\": 56,\n",
      "    \"B-IPV4\": 91,\n",
      "    \"B-IPV6\": 95,\n",
      "    \"B-JOBAREA\": 20,\n",
      "    \"B-JOBDESCRIPTOR\": 111,\n",
      "    \"B-JOBTITLE\": 28,\n",
      "    \"B-JOBTYPE\": 24,\n",
      "    \"B-LASTNAME\": 4,\n",
      "    \"B-LITECOINADDRESS\": 78,\n",
      "    \"B-MAC\": 97,\n",
      "    \"B-MASKEDNUMBER\": 85,\n",
      "    \"B-MIDDLENAME\": 62,\n",
      "    \"B-NEARBYGPSCOORDINATE\": 100,\n",
      "    \"B-ORDINALDIRECTION\": 84,\n",
      "    \"B-PASSWORD\": 74,\n",
      "    \"B-PHONEIMEI\": 9,\n",
      "    \"B-PHONENUMBER\": 52,\n",
      "    \"B-PHONE_NUMBER\": 118,\n",
      "    \"B-PIN\": 69,\n",
      "    \"B-PREFIX\": 0,\n",
      "    \"B-SECONDARYADDRESS\": 32,\n",
      "    \"B-SEX\": 54,\n",
      "    \"B-SSN\": 109,\n",
      "    \"B-STATE\": 17,\n",
      "    \"B-STREET\": 30,\n",
      "    \"B-STREETADDRESS\": 114,\n",
      "    \"B-SUFFIX\": 116,\n",
      "    \"B-TIME\": 8,\n",
      "    \"B-URL\": 18,\n",
      "    \"B-USERAGENT\": 38,\n",
      "    \"B-USERNAME\": 11,\n",
      "    \"B-VEHICLEVIN\": 102,\n",
      "    \"B-VEHICLEVRM\": 65,\n",
      "    \"B-ZIPCODE\": 87,\n",
      "    \"I-ACCOUNTNAME\": 41,\n",
      "    \"I-ACCOUNTNUMBER\": 43,\n",
      "    \"I-AGE\": 37,\n",
      "    \"I-AMOUNT\": 47,\n",
      "    \"I-BIC\": 90,\n",
      "    \"I-BITCOINADDRESS\": 61,\n",
      "    \"I-BUILDINGNUMBER\": 83,\n",
      "    \"I-CITY\": 16,\n",
      "    \"I-COMPANYNAME\": 27,\n",
      "    \"I-COMPANY_NAME\": 113,\n",
      "    \"I-COUNTY\": 35,\n",
      "    \"I-CREDITCARDCVV\": 73,\n",
      "    \"I-CREDITCARDISSUER\": 55,\n",
      "    \"I-CREDITCARDNUMBER\": 50,\n",
      "    \"I-CURRENCY\": 72,\n",
      "    \"I-CURRENCYCODE\": 81,\n",
      "    \"I-CURRENCYNAME\": 77,\n",
      "    \"I-CURRENCYSYMBOL\": 45,\n",
      "    \"I-DATE\": 7,\n",
      "    \"I-DOB\": 68,\n",
      "    \"I-EMAIL\": 23,\n",
      "    \"I-ETHEREUMADDRESS\": 59,\n",
      "    \"I-EYECOLOR\": 105,\n",
      "    \"I-FIRSTNAME\": 3,\n",
      "    \"I-FULLNAME\": 121,\n",
      "    \"I-GENDER\": 14,\n",
      "    \"I-HEIGHT\": 107,\n",
      "    \"I-IBAN\": 64,\n",
      "    \"I-IP\": 57,\n",
      "    \"I-IPV4\": 92,\n",
      "    \"I-IPV6\": 96,\n",
      "    \"I-JOBAREA\": 93,\n",
      "    \"I-JOBTITLE\": 29,\n",
      "    \"I-JOBTYPE\": 99,\n",
      "    \"I-LASTNAME\": 25,\n",
      "    \"I-LITECOINADDRESS\": 79,\n",
      "    \"I-MAC\": 98,\n",
      "    \"I-MASKEDNUMBER\": 86,\n",
      "    \"I-MIDDLENAME\": 94,\n",
      "    \"I-NEARBYGPSCOORDINATE\": 101,\n",
      "    \"I-PASSWORD\": 75,\n",
      "    \"I-PHONEIMEI\": 10,\n",
      "    \"I-PHONENUMBER\": 53,\n",
      "    \"I-PHONE_NUMBER\": 119,\n",
      "    \"I-PIN\": 70,\n",
      "    \"I-PREFIX\": 1,\n",
      "    \"I-SECONDARYADDRESS\": 33,\n",
      "    \"I-SSN\": 110,\n",
      "    \"I-STATE\": 108,\n",
      "    \"I-STREET\": 31,\n",
      "    \"I-STREETADDRESS\": 115,\n",
      "    \"I-SUFFIX\": 117,\n",
      "    \"I-TIME\": 21,\n",
      "    \"I-URL\": 19,\n",
      "    \"I-USERAGENT\": 39,\n",
      "    \"I-USERNAME\": 12,\n",
      "    \"I-VEHICLEVIN\": 103,\n",
      "    \"I-VEHICLEVRM\": 66,\n",
      "    \"I-ZIPCODE\": 88,\n",
      "    \"O\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"google/electra-large-generator\", num_labels=len(all_labels), label2id=label2id, id2label=id2label)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36bb2c00-a05e-41ca-b60c-e71bc6cdb6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"electra-large-generator_finetuned_ai4privacy\",\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    overwrite_output_dir=True,\n",
    "    warmup_ratio=0.2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    lr_scheduler_type='cosine_with_restarts',\n",
    "    report_to='wandb',\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"test\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b47741cd-e504-4c48-b1c8-b466f2d8c064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msripaadsrinivasan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sripaadsrinivasan/Projects/ai4privacy/notebooks/wandb/run-20231020_160741-rte30ow0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sripaadsrinivasan/huggingface/runs/rte30ow0' target=\"_blank\">golden-cosmos-1041</a></strong> to <a href='https://wandb.ai/sripaadsrinivasan/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sripaadsrinivasan/huggingface' target=\"_blank\">https://wandb.ai/sripaadsrinivasan/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sripaadsrinivasan/huggingface/runs/rte30ow0' target=\"_blank\">https://wandb.ai/sripaadsrinivasan/huggingface/runs/rte30ow0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='5089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  29/5089 02:55 < 9:08:36, 0.15 it/s, Epoch 0.04/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 5.52 GB, other allocations: 3.45 GB, max allowed: 9.07 GB). Tried to allocate 119.23 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/transformers/trainer.py:1971\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1969\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1972\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/accelerate/optimizer.py:145\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/torch/optim/adamw.py:464\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    462\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    466\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 5.52 GB, other allocations: 3.45 GB, max allowed: 9.07 GB). Tried to allocate 119.23 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "test_result = trainer.evaluate(tokenized_dataset['test'])\n",
    "\n",
    "train_metrics = train_result.metrics\n",
    "test_metrics = test_result.metrics\n",
    "\n",
    "max_train_samples = len(tokenized_dataset['train'])\n",
    "max_eval_samples = len(tokenized_dataset['test'])\n",
    "\n",
    "train_metrics[\"train_samples\"] = min(max_train_samples, len(tokenized_dataset['train']))\n",
    "trainer.log_metrics(\"train\", train_metrics)\n",
    "\n",
    "test_metrics[\"eval_samples\"] = min(max_eval_samples, len(tokenized_dataset['test']))\n",
    "trainer.log_metrics(\"eval\", test_metrics)\n",
    "\n",
    "trainer.save_metrics(\"train\", train_metrics)\n",
    "trainer.save_metrics(\"eval\", test_metrics)\n",
    "\n",
    "trainer.save_state()\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7d6ef-2cfc-4d3a-89a2-13d962203194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
