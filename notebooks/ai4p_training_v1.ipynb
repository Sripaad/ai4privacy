{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04c7bb-f80a-47ad-a655-69f396233278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974bf8e-286e-4309-94e6-d82899d67ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_json(\"data/english_balanced_10k.jsonl\", lines=True)\n",
    "df2 = pd.read_json(\"data/PII43k_original.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa9d87-2e12-4542-805a-a66722f8a8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df1.append([df2]).reset_index(drop=True)\n",
    "df = df.rename(columns={\n",
    "    \"Masked text\" : \"target_text\",\n",
    "    \"Unmasked text\": \"source_text\",\n",
    "    \"Tokenised Masked text\": \"tokenized_text\",\n",
    "    \"Tokensised Unmasked text\": \"ner_tags\", # need it to find all present labels\n",
    "}) # why add spaces in column names?\n",
    "# df = df.drop_duplicates()\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea950da0-442d-4878-a39b-b910ebb3472f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37ec40-2be2-4d0d-a273-0d6bc433d3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nt = df.ner_tags.tolist()\n",
    "nt = list(itertools.chain.from_iterable(nt)) # merge the list of lists into one list\n",
    "nt = collections.Counter(nt) # Get count of each tag\n",
    "all_labels = list(nt.keys()) # get all unique tags(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136e5f7-672e-46cd-9968-a0d4c4f64d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_texts = df.source_text.tolist()\n",
    "target_texts = df.target_text.tolist()\n",
    "tokenized_texts = df.tokenized_text.tolist()\n",
    "ner_tags = df.ner_tags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7b39c-1cc9-4e25-9700-aa9ce6c3fd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d7576-4c49-47d1-aec3-19199981e606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283387ee-e978-449d-9492-237b0eb2f957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a533b-f9da-408f-84e8-f541d6ac9c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # they have mentioned it in their HF datasets repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced38fcf-014d-4142-959a-048e5a1b8eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## checking if the tokens align\n",
    "i = random.randint(0, len(source_texts))\n",
    "x0 = tokenizer.convert_ids_to_tokens(tokenizer(source_texts[i])['input_ids'])\n",
    "x0.pop(0)  # CLS is not present in the dataset\n",
    "for t in zip(x0, tokenized_texts[i]):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dcbb1-e25e-4e2e-bec9-0e4e7520487d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking if the tokens align with tags\n",
    "# i = random.randint(0, len(target_texts))\n",
    "for t in zip(ner_tags[i], tokenized_texts[i]):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3848bd-bac9-4816-a7cf-a98f6aef36c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create label dict\n",
    "label2id = dict([(value,key) for key, value in enumerate(all_labels)])\n",
    "id2label = dict(map(reversed, label2id.items()))\n",
    "\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6bedb7-1960-49e5-b722-230a7746d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in tqdm(range(len(ner_tags))):\n",
    "    tags = ner_tags[j]\n",
    "    for i in range(len(tags)):\n",
    "        for k,v in label2id.items():\n",
    "            if tags[i] == k:\n",
    "                tags[i] = v\n",
    "    ner_tags[j] = tags\n",
    "df.ner_tags = ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740458df-191c-4e02-8089-aa370b0e5fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ner_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23a079-cdce-4818-a27f-b55f855f5f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['source_words']] = \"source_words\"\n",
    "source_words = [text.split(\" \") for text in source_texts]\n",
    "df.source_words = source_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8d23e-6507-4f44-929a-52d3bb70cbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing rows where the len(tokenized_texts[i]) does not match len(ner_tags[i])\n",
    "idx = [i for i in range(len(ner_tags)) if len(tokenized_texts[i]) != len(ner_tags[i])]\n",
    "df = df.drop(index=idx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cc0ae-0742-4a8d-91fe-61a35ec33603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e9dd6-5e09-48d7-9d09-529bc33af07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_labels(example):\n",
    "    tokenized_input = tokenizer(example[\"tokenized_text\"], is_split_into_words=True)\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    aligned_labels = [-100 if i is None else example[\"ner_tags\"][i] for i in word_ids]\n",
    "    tokenized_input['labels'] = aligned_labels\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8f2d7-4180-4aee-ad80-badf03240963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "al = align_labels(dataset[0])\n",
    "print(len(al['input_ids']), len(al['attention_mask']), len(al['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d678b-c154-4f41-8f0b-83f0a0b08fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokenized_text\"], is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6e964-d915-488b-b03a-cf262096cb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = dataset.map(align_labels, num_proc=8, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5d4c6-5d52-4612-bce9-0c65a98181b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = x.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8732f21-bcd6-47bb-839a-13e66b84c6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ae609-8ce5-4521-b8e5-f707267e7b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3f6fa-b381-4719-bbc5-556edf2731a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = datasets.load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "    for k in results.keys():\n",
    "        if (k not in flattened_results.keys()):\n",
    "            flattened_results[f\"{k}_f1\"] = results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027a465-848e-4be9-aa5d-1efd90aad2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(all_labels), label2id=label2id, id2label=id2label)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb2c00-a05e-41ca-b60c-e71bc6cdb6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert_finetuned_ai4privacy\",\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    overwrite_output_dir=True,\n",
    "    warmup_ratio=0.2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    lr_scheduler_type='linear',\n",
    "    report_to='wandb',\n",
    "    push_to_hub=True,\n",
    "    hub_token=\"TOKEN\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"test\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47741cd-e504-4c48-b1c8-b466f2d8c064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "test_result = trainer.evaluate(tokenized_dataset['test'])\n",
    "\n",
    "train_metrics = train_result.metrics\n",
    "test_metrics = test_result.metrics\n",
    "\n",
    "max_train_samples = len(tokenized_dataset['train'])\n",
    "max_eval_samples = len(tokenized_dataset['test'])\n",
    "\n",
    "train_metrics[\"train_samples\"] = min(max_train_samples, len(tokenized_dataset['train']))\n",
    "trainer.log_metrics(\"train\", train_metrics)\n",
    "\n",
    "test_metrics[\"eval_samples\"] = min(max_eval_samples, len(tokenized_dataset['test']))\n",
    "trainer.log_metrics(\"eval\", test_metrics)\n",
    "\n",
    "trainer.save_metrics(\"train\", train_metrics)\n",
    "trainer.save_metrics(\"eval\", test_metrics)\n",
    "\n",
    "trainer.save_state()\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7d6ef-2cfc-4d3a-89a2-13d962203194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
